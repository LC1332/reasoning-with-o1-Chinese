# Reasoning with o1: Related Research Papers

## 2024 Papers

### Core o1 Papers
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/)
  - Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  - OpenAI

- [From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond](https://arxiv.org/abs/2411.03590)
  - Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz
  - Microsoft Research
  - Keywords: medical AI, LLM reasoning, run-time strategies

### Reasoning & Chain-of-Thought
- [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/)
  - Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
  - Code: [GitHub Repository](https://github.com/ezelikman/quiet-star)

- [MathCritique: Enhancing LLM Reasoning via Critique Models](https://arxiv.org/)
  - Zhiheng Xi, et al.
  - Website: [https://mathcritique.github.io/](https://mathcritique.github.io/)

## 2023 Papers

### Foundational Papers
- [Training Chain-of-Thought via Latent-Variable Inference](https://arxiv.org/)
  - Du Phan, Matthew D. Hoffman, David Dohan, et al.
  - Google Research

- [Certified reasoning with language models](https://arxiv.org/)
  - Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D. Goodman
  - Stanford University

### OpenAI Contributions
- [Deliberative alignment: reasoning enables safer language models](https://arxiv.org/)
  - OpenAI Team
  - OpenAI

- [Training Verifiers to Solve Math Word Problems](https://arxiv.org/)
  - Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al.
  - OpenAI

## Key Research Areas

### Self-Improvement & Training
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/)
  - Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, et al.
  - DeepMind

### Planning & Search
- [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training](https://arxiv.org/)
  - Xidong Feng, Ziyu Wan, Muning Wen, et al.
  - UCL, Imperial College London

### Evaluation & Benchmarking
- [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://arxiv.org/)
  - Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, et al.
  - Microsoft Research, MIT

## How to Contribute
To add a new paper to this list:
1. Follow the existing format
2. Include links to paper, code, and additional resources when available
3. Create a pull request with your addition

## Note
This is a living document that will be updated as new research emerges in the field of LLM reasoning and o1-related developments.